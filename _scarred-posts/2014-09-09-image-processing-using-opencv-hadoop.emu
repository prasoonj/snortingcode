|> Metadata
    title = Image Processing using OpenCV on a Hadoop cluster using HIPI
    tags = hadoop mapreduce hipi opencv

|> List
    1.  Use HIPI to create an image bundle by feeding images from a social media platform, etc. You can use the Downloader example provided by HIPI to do that which expects a file with the URLs of all the images that need to be downloaded - one on each line (if you don’t want to provide your own implementations for InputFormat and RecordReader classes!). Go through this link to read more about the Downloader example. At the end of this step, you would have a HIB (HIPI Image Bundle) file, which is a collection (archive) of all the images that were downloaded. This step basically ensures that our MapReduce program gets a big file to work with which is what hadoop is meant to be dealing with.

    -- Image bundle as input to the mapper class that splits the HIB file into FloatImages that are the input to the map function of the mapper class. 
        1. I used OpenCv’s java bindings (available post version 2.4.9) to detect human faces in the images. This step would require you to convert the FloatImage to OpenCv’s Mat (which is essentially a matrix class with a header section detailing information about the image) - One of many Mat constructors takes a /float[]/ that we can get from /FloatImage.getData()/ Tip: The imageType produced from a float[] based constructor would be /Cv_32FC3/. Be sure to convert it to /Cv_8UC3/ which is required by the next steps. 
        
        -- I used the example on the OpenCv documentation page to do face detection. Here’s the relevant, slightly modified code:
        |> Code
            lang = shell
            code = 
                $ sudo -s
                //Create a face detector from the cascade file in the resources directory
                CascadeClassifier faceDetector = new CascadeClassifier("lbpcascade_frontface.xml");
                
                //Detect faces in the imageMatOfRect 
                faceDetections = new MatOfRect();
                faceDetector.detectMultiScale(imageMat, faceDetections);

    -- The reducer is used simply to add the number of times faces are detected to give a total count (pretty silly, I know!)

|> H3
    Caveats and Tips:

Don’t forget to load the native OpenCv libraries before you start using them. For that you’d be required to place the lib files in the distributed cache and load them in the setup(Context c) method of the Mapper class so that they are available on each node of the cluster.
Create the Mat instance using proper type. Do required conversions.

The HIPI library might not work properly if you are on hadoop-2 (if you get an error similar to “Found interface but class expected” you are using a jar compiled with hadoop-1). I had to change a few of the hipi classes to get it to work. Will try to post the changes on my fork of hipi asap.

2a: Use image processing to add tags to the image.
2b: Pass the output of 2a through another mapper that makes ‘tag_pairs’ (thus defining tags-correlation) and passes it to the reduce method.
/([tag1, tag2],1)/

|> H4
    Reducer

2c: The reducer will be able to predict a high-correlation between 2 (or, hopefully, more) tags based on the total number of times the same pair of tags are encountered in different images.Catch: The order of tags should not matter while adding up the counts.

|> H4
    Enhancement: 

The pair of tags can be input to another mapper/reducer for the same image by keeping on adding more tags to the pair, etc.

Step 3:Take an image as input and predict the associated tags.