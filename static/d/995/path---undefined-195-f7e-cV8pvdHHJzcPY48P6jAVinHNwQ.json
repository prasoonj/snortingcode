{"data":{"site":{"siteMetadata":{"title":"Snorting Code, Et al.","subtitle":"I don't take myself too seriously!","copyright":"© All rights reserved.","author":{"name":"Prasoon","twitter":"prasoonj"},"disqusShortname":"","url":"https://prasoonj.github.io/snortingcode"}},"markdownRemark":{"id":"ffc840c5-7a53-5e82-9972-140102d8986b","html":"<p>This is the third machine I’m setting up with a pseudo-hadoop cluster and each time I have to look up different tutorials to get everything right. High time I wrote it all down (neatly) in my own little corner of the Internet as a refresher for the next time I have to do the same (will try to create –and test– a shell script that makes that as simple as executing a script).\n$ sudo addgroup hadoop$ sudo adduser hduser(provide password and other details)\nQuite clearly not a mandatory step but it would help to keep the system wide things out of the way of hadoop. Also, this is a good way to simulate the production environment - with no root permissions, and elevated priviledges, etc.\n$ su - hduser$ ssh-keygen -t rsa -P \"\"\nSwitch to the ‘hduser’ user and generate a public/private rsa key pair. Hadoop uses SSH to connect with other nodes, make sure to use no password while creating the key pair. Accept defaults to save the key at ~/.ssh/id<em>rsa \nAdd the newly created key to the authorized</em>keys:\n$ cat ~/.ssh/id<em>rsa.pub >> ~/.ssh/authorized</em>keys$ ssh localhost\nAdd ”localhost’ permanetly to the list of ”known<em>hosts’.\nDownload the latest stable release of Hadoop 2 (make sure that the ”hduser’ is in the list of sudoers or do these tasks by switching to another user and later give proper permissions to hduser):\n$ sudo mkdir -p /opt/hadoop2.6$ wget <a href=\"http://mirrors.gigenet.com/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz\">http://mirrors.gigenet.com/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz</a> -P /opt/hadoop2.6$ cd /opt/hadoop2.6$ sudo tar xzf hadoop-2.6.0.tar.gz\nfileNow may be a good time to set a few environment variable so that we don’t have to type the entire path to use hadoop. Open your ~/.bashrc file and add these lines at the end (as indicated in this guide):\nexport HADOOP</em>PREFIX=/opt/hadoop2.6/hadoop-2.6.0export HADOOP<em>HOME=$HADOOP</em>PREFIXexport HADOOP<em>COMMON</em>HOME=$HADOOP<em>PREFIX\nexport HADOOP</em>CONF<em>DIR=$HADOOP</em>PREFIX/etc/hadoop\nexport HADOOP<em>HDFS</em>HOME=$HADOOP<em>PREFIX\nexport HADOOP</em>MAPRED<em>HOME=$HADOOP</em>PREFIX\nexport HADOOP<em>YARN</em>HOME=$HADOOP<em>PREFIX\nNext, it’s time to tell hadoop which java to use. This is the only required configuration that you need to do according to the hadoop official documentation. Add the $JAVA</em>HOME to the $HADOOP<em>HOME/etc/hadoop/hadoop-env.sh file.\nThere are a bunch of configuration files under $HADOOP</em>HOME/etc/hadoop and it is a good idea to understand what each of it does, not because you would be needing all of them in order to get started but, it might save you some time later when you are trying to figure out how, for instance, the replication factor can be changed for the cluster.\nThe important ones are -</p>\n<h1>in the file core-site.xml</h1>\n<configuration>\n{% highlight python %}\n    <name>fs.defaultFS</name>\n    <value>hdfs://localhost:9000</value>\n{% endhighlight %}\n</configuration># in the hdfs-site.xml\n<configuration>\n{% highlight python %}\n    <name>dfs.replication</name>\n    <value>1</value>\n{% endhighlight %}\n</configuration>\nIt’s time now to format the file-system:\n$ $HADOOP_HOME/bin/hdfs namenode -format\nWe are nearly there! Start the deamons required for hadoop and create directories required to run M/R jobs:\n$ HADOOP_HOME/sbin/start-dfs.sh$ hadoop fs -mkdir -p /user/hduser \nThis will start the ''NameNode’, ''DataNode’ and ''SecondaryNameNode’ on your pseudo-cluster. Use the utility ''jps’ to see if they are started properly.\n[TODO: Will add the yarn specific configuration steps to this soon.]","fields":{"tagSlugs":["/tags/hadoop/","/tags/installation/","/tags/configuration/","/tags/mapreduce/"]},"frontmatter":{"title":"Hadoop Installation Guide.","tags":["hadoop","installation","configuration","mapreduce"],"date":"2015-01-27T01:54:00+05:30","description":null}}},"pageContext":{"slug":"/undefined/"}}