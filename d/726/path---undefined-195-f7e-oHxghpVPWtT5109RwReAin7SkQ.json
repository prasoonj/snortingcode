{"data":{"site":{"siteMetadata":{"title":"Snorting Code, Et al.","subtitle":"I don't take myself too seriously!","copyright":"© All rights reserved.","author":{"name":"Prasoon","twitter":"prasoonj"},"disqusShortname":"","url":"https://prasoonj.github.io/snortingcode"}},"markdownRemark":{"id":"0c34e095-4abb-5c4a-83e3-4d954f9a9dbb","html":"<p>Step 1:Use HIPI to create an image bundle by feeding images from a social media platform, etc. You can use the Downloader example provided by HIPI to do that which expects a file with the URLs of all the images that need to be downloaded - one on each line (if you don’t want to provide your own implementations for InputFormat and RecordReader classes!). Go through this link to read more about the Downloader example. \nAt the end of this step, you would have a HIB (HIPI Image Bundle) file, which is a collection (archive) of all the images that were downloaded. This step basically ensures that our MapReduce program gets a big file to work with which is what hadoop is meant to be dealing with.\nStep 2: Image bundle as input to the mapper class that splits the HIB file into FloatImages that are the input to the map function of the mapper class. \n2a:I used OpenCv’s java bindings (available post version 2.4.9) to detect human faces in the images. This step would require you to convert the FloatImage to OpenCv’s Mat (which is essentially a matrix class with a header section detailing information about the image) - One of many Mat constructors takes a float[] that we can get from\nFloatImage.getData()\nTip: The imageType produced from a float[] based constructor would be Cv<em>32FC3. Be sure to convert it to Cv</em>8UC3 which is required by the next steps.\n2b:I used the example on the OpenCv documentation page to do face detection. Here’s the relevant, slightly modified code:\nsudo -s//Create a face detector from the cascade file in the resources directoryCascadeClassifier faceDetector = new CascadeClassifier(“lbpcascade<em>frontface.xml”);//Detect faces in the imageMatOfRect faceDetections = new MatOfRect();faceDetector.detectMultiScale(imageMat, faceDetections);\nStep 3:The reducer is used simply to add the number of times faces are detected to give a total count (pretty silly, I know!)\nCaveats and Tips:\nDon’t forget to load the native OpenCv libraries before you start using them. For that you’d be required to place the lib files in the distributed cache and load them in the setup(Context c) method of the Mapper class so that they are available on each node of the cluster.\nCreate the Mat instance using proper type. Do required conversions.\nThe HIPI library might not work properly if you are on hadoop-2 (if you get an error similar to “Found interface but class expected” you are using a jar compiled with hadoop-1). I had to change a few of the hipi classes to get it to work. Will try to post the changes on my fork of hipi asap.\n2a: Use image processing to add tags to the image.\n2b: Pass the output of 2a through another mapper that makes ‘tag</em>pairs’ (thus defining tags-correlation) and passes it to the reduce method.\n([tag1, tag2],1)\nReducer\n2c: The reducer will be able to predict a high-correlation between 2 (or, hopefully, more) tags based on the total number of times the same pair of tags are encountered in different images.Catch: The order of tags should not matter while adding up the counts.\nEnhancement: The pair of tags can be input to another mapper/reducer for the same image by keeping on adding more tags to the pair, etc.\nStep 3:Take an image as input and predict the associated tags.</p>","fields":{"tagSlugs":["/tags/hadoop/","/tags/mapreduce/","/tags/image-processing/","/tags/hipi/","/tags/opencv/"]},"frontmatter":{"title":"(Notes on) Image Processing using OpenCV on a Hadoop cluster using HIPI","tags":["hadoop","mapreduce","image processing","hipi","opencv"],"date":"2014-09-09T03:02:00+05:30","description":null}}},"pageContext":{"slug":"/undefined/"}}